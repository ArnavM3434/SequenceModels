# Sequence Models Assignments (Coursera)

This repository contains assignments completed as part of the **Sequence Models** course on Coursera. The projects cover a range of architectures including RNNs, LSTMs, attention mechanisms, and Transformers.

---

##  RNNs & LSTMs

- Implemented a vanilla RNN and LSTM from scratch using **NumPy**.
- Built a **character-level language model** to generate dinosaur names.
- Developed a **jazz music generation model** using an LSTM in **TensorFlow/Keras**.

---

##  Word Embeddings

- Used **GloVe word embeddings** with an LSTM in **Keras** to build a model that maps sentences to emojis, capturing the **sentiment** of the input.

---

##  Pre- & Post-Attention LSTMs

- Built a model that converts human-readable dates (e.g., `"Saturday, May 16th 2025"`) into machine-readable format (`"05-16-2025"`).
- Architecture: **Bi-directional LSTM** (pre-attention) → **Attention Layer** → **Unidirectional LSTM** (post-attention), all in **Keras**.

---

##  Transformer

- Built a **Transformer model** from scratch in **Keras**.
- Implemented core components:
  - Positional Encoding
  - Multi-Head Attention
  - Padding and Look-Ahead Masks
  - Layer Normalization
  - Residual Connections

---

These assignments demonstrate key deep learning sequence modeling techniques and their practical applications.
